import requests
from bs4 import BeautifulSoup
import google.generativeai as genai
import yaml
from datetime import datetime
import re
import argparse
import sys
from urllib.parse import urlparse, urljoin
import json
import time
import os
from dotenv import load_dotenv

class ProductScraper:
    def __init__(self):
        
        load_dotenv()
        
        self.gemini_api_key = os.getenv('GEMINI_API_KEY')
        if not self.gemini_api_key:
            raise ValueError("‚ùå GEMINI_API_KEY non trouv√©e dans le fichier .env")
        
        genai.configure(api_key=self.gemini_api_key)
        self.model = genai.GenerativeModel('gemini-2.0-flash')
        
        # Headers pour les requ√™tes HTTP
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def load_urls_from_file(self, filename='urlfiche.txt'):
        
        try:
            if not os.path.exists(filename):
                print(f"‚ùå Fichier {filename} non trouv√©.")
                return []
            
            with open(filename, 'r', encoding='utf-8') as f:
                urls = []
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    
                    # Ignorer les lignes vides et les commentaires
                    if not line or line.startswith('#'):
                        continue
                    
                    
                    try:
                        result = urlparse(line)
                        if all([result.scheme, result.netloc]):
                            urls.append(line)
                            print(f"‚úÖ URL {line_num}: {line}")
                        else:
                            print(f"‚ö†Ô∏è  URL {line_num} invalide ignor√©e : {line}")
                    except Exception:
                        print(f"‚ö†Ô∏è  URL {line_num} invalide ignor√©e : {line}")
                
                return urls
                
        except Exception as e:
            print(f"‚ùå Erreur lors de la lecture du fichier {filename}: {e}")
            return []

    def scrape_article(self, url):
        
        try:
            print(f"üì• Scraping de l'article : {url}")
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extraction du contenu principal
            content = self._extract_main_content(soup)
            title = self._extract_title(soup)
            image_url = self._extract_product_image(soup)
            
            return {
                'url': url,
                'title': title,
                'content': content,
                'image_url': image_url,
                'raw_html': str(soup)
            }
            
        except requests.RequestException as e:
            print(f"‚ùå Erreur lors du scraping de {url}: {e}")
            return None
        except Exception as e:
            print(f"‚ùå Erreur inattendue pour {url}: {e}")
            return None

    def _extract_title(self, soup):
        
        selectors = ['h1', 'title', '.article-title', '.post-title', '#title']
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element and element.get_text().strip():
                return element.get_text().strip()
        
        return "Titre non trouv√©"

    def _extract_main_content(self, soup):
        
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement']):
            element.decompose()
        
        content_selectors = [
            'article', '.article-content', '.post-content', '.entry-content',
            '.content', 'main', '#content', '.article-body', '.post-body'
        ]
        
        for selector in content_selectors:
            content_element = soup.select_one(selector)
            if content_element:
                return content_element.get_text(separator=' ', strip=True)
        
        body = soup.find('body')
        if body:
            return body.get_text(separator=' ', strip=True)
        
        return soup.get_text(separator=' ', strip=True)

    def _extract_product_image(self, soup):
        """Extrait l'URL de l'image principale du produit."""
        # S√©lecteurs sp√©cifiques aux images de produits
        image_selectors = [
            # S√©lecteurs Open Graph et Twitter
            'meta[property="og:image"]',
            'meta[name="twitter:image"]',
            # S√©lecteurs sp√©cifiques aux sites e-commerce
            '#landingImage',  # Amazon
            '#main-image',    # Commun
            '.product-image-main img',
            '.product-featured-image',
            '.gallery-image--default',
            '[data-main-image]',
            # S√©lecteurs g√©n√©riques pour images de produits
            '.product-image img',
            '.primary-image',
            '.main-product-image',
            # Fallback sur premi√®re image pertinente
            'img[itemprop="image"]',
            '.product img:first-of-type'
        ]
        
        for selector in image_selectors:
            element = soup.select_one(selector)
            if element:
                # Extraire l'URL selon le type d'√©l√©ment
                if element.name == 'meta':
                    image_url = element.get('content')
                else:
                    # Chercher d'abord data-src pour les images lazy-loaded
                    image_url = element.get('data-src') or element.get('src')
                
                if image_url:
                    # Nettoyer l'URL
                    image_url = image_url.split('?')[0]  # Retirer les param√®tres
                    # S'assurer que l'URL est absolue
                    if not image_url.startswith(('http://', 'https://')):
                        base_url = soup.find('base', href=True)
                        if base_url:
                            image_url = urljoin(base_url['href'], image_url)
                    return image_url
        
        return None

    def generate_product_sheet(self, article_data):
        """G√©n√®re une fiche produit √† partir d'UN SEUL article"""
        try:
            print(f"ü§ñ G√©n√©ration de la fiche produit avec Gemini pour: {article_data['url']}")
            
            prompt = self._create_gemini_prompt(article_data)
            
            response = self.model.generate_content(prompt)
            
            if not response or not response.text:
                raise Exception("R√©ponse vide de l'API Gemini")
            
            product_data = self._parse_gemini_response(response.text)
            markdown_content = self._generate_markdown(product_data)
            
            return markdown_content
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration avec Gemini: {e}")
            return None

    def _create_gemini_prompt(self, article_data):
        """Cr√©e le prompt pour UN SEUL article"""
        
        prompt = f"""
Tu es un expert en r√©daction de fiches produits techniques. √Ä partir de l'article suivant, tu dois extraire les informations d'un produit et cr√©er une fiche produit EXACTEMENT dans ce format JSON (respecte scrupuleusement la structure) :

{{
    "name": "Nom complet du produit avec toutes ses caract√©ristiques",
    "brand": "Marque du produit",
    "model": "Mod√®le exact du produit",
    "image": "{article_data.get('image_url', '')}",
    "amazonASIN": "ASIN_PLACEHOLDER",
    "publishDate": "{datetime.now().strftime('%Y-%m-%d')}",
    "updateDate": "{datetime.now().strftime('%Y-%m-%d')}",
    "draft": false,
    "title": "Titre accrocheur pour le test/avis du produit",
    "hookIntro": "Introduction accrocheuse d√©crivant les points forts du produit",
    "keyBenefits": [
        "B√©n√©fice 1 : Description d√©taill√©e du premier avantage",
        "B√©n√©fice 2 : Description d√©taill√©e du deuxi√®me avantage",
        "B√©n√©fice 3 : Description d√©taill√©e du troisi√®me avantage",
        "B√©n√©fice 4 : Description d√©taill√©e du quatri√®me avantage",
        "B√©n√©fice 5 : Description d√©taill√©e du cinqui√®me avantage"
    ],
    "keyFeatures": [
        "Caract√©ristique technique 1",
        "Caract√©ristique technique 2", 
        "Caract√©ristique technique 3",
        "Caract√©ristique technique 4",
        "Caract√©ristique technique 5",
        "Caract√©ristique technique 6",
        "Caract√©ristique technique 7",
        "Caract√©ristique technique 8"
    ],
    "detailedSpecs": "Description d√©taill√©e et technique du produit mettant en avant ses sp√©cifications et performances",
    "ctaText": "Voir les Offres pour le [Nom du Produit]",
    "affiliateLink": "https://www.amazon.fr/dp/ASIN_PLACEHOLDER?tag=votretag-21",
    "category": "Cat√©gorie appropri√©e du produit",
    "tags": ["tag1", "tag2", "tag3", "tag4", "tag5", "tag6", "tag7", "tag8"]
}}

ARTICLE √Ä ANALYSER:
URL: {article_data['url']}
Titre: {article_data['title']}
Contenu: {article_data['content'][:4000]}...

INSTRUCTIONS IMPORTANTES:
1. Extrait UNIQUEMENT les informations du produit principal mentionn√© dans cet article
2. Assure-toi que le nom du produit soit complet et d√©taill√©
3. Les keyBenefits doivent suivre le format "Titre : Description"
4. Les keyFeatures doivent √™tre des caract√©ristiques techniques pr√©cises
5. La cat√©gorie doit √™tre pertinente (ex: "Moniteurs Gaming", "Smartphones", "Casques Audio", etc.)
6. Les tags doivent inclure la marque, le mod√®le et des mots-cl√©s pertinents
7. R√©ponds UNIQUEMENT avec le JSON, sans texte suppl√©mentaire
"""
        
        return prompt

    def _parse_gemini_response(self, response_text):
        try:
            response_text = response_text.strip()
            
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                raise Exception("Aucun JSON trouv√© dans la r√©ponse")
            
            json_text = response_text[json_start:json_end]
            product_data = json.loads(json_text)
            
            return product_data
            
        except json.JSONDecodeError as e:
            print(f"‚ùå Erreur de parsing JSON: {e}")
            print(f"R√©ponse brute: {response_text[:500]}...")
            return None
        except Exception as e:
            print(f"‚ùå Erreur lors du parsing: {e}")
            return None

    def _generate_markdown(self, product_data):
        if not product_data:
            return None
        
        # Template markdown exact selon le format sp√©cifi√©
        markdown_template = f"""---
# --- Informations de Base sur le Produit ---
name: '{product_data.get("name", "")}'
brand: '{product_data.get("brand", "")}'
model: '{product_data.get("model", "")}'
image: '{product_data.get("image", "https://via.placeholder.com/600x400")}' # Placeholder image
amazonASIN: '{product_data.get("amazonASIN", "PRODUCT_ASIN_PLACEHOLDER")}' # Placeholder ASIN
publishDate: {product_data.get("publishDate", datetime.now().strftime('%Y-%m-%d'))}
updateDate: {product_data.get("updateDate", datetime.now().strftime('%Y-%m-%d'))}
draft: {str(product_data.get("draft", False)).lower()}

# --- √âtape 2 : Accroche ---
title: '{product_data.get("title", "")}'
hookIntro: '{product_data.get("hookIntro", "")}'

# --- √âtape 3 : B√©n√©fices Cl√©s ---
keyBenefits:"""
        
        # Ajouter les b√©n√©fices
        for benefit in product_data.get("keyBenefits", []):
            markdown_template += f"\n  - '{benefit}'"
        
        markdown_template += f"""

# --- √âtape 4 : Caract√©ristiques Pertinentes ---
keyFeatures:"""
        
        # Ajouter les caract√©ristiques
        for feature in product_data.get("keyFeatures", []):
            markdown_template += f"\n  - '{feature}'"
        
        markdown_template += f"""
detailedSpecs: '{product_data.get("detailedSpecs", "")}'

# --- √âtape 6 : Appel √† l'Action ---
ctaText: '{product_data.get("ctaText", "")}'
affiliateLink: '{product_data.get("affiliateLink", "")}' # Placeholder link

# --- √âtape 7 : Optimisation & Cat√©gorisation ---
category: '{product_data.get("category", "")}'
tags: {product_data.get("tags", [])}

---

## Le {product_data.get("brand", "")} {product_data.get("model", "")} : Immersion et Performance au Rendez-vous

Plongez au c≈ìur de l'action avec le **{{frontmatter.name}}**. Ce produit est une invitation √† red√©couvrir vos exp√©riences avec une qualit√© et une performance √©poustouflantes. Comme nous l'avons soulign√© : **{{frontmatter.hookIntro}}**

### Atouts Majeurs pour une Exp√©rience In√©gal√©e

Le {product_data.get("brand", "")} {product_data.get("model", "")} est con√ßu pour la performance :

{{frontmatter.keyBenefits.map((benefit) => (
  - **${{benefit.split(':')[0].trim()}} :** ${{benefit.split(':')[1].trim()}}
))}}

### Sp√©cifications Techniques D√©taill√©es

Les caract√©ristiques techniques de ce produit parlent d'elles-m√™mes :

{{frontmatter.keyFeatures.map((feature) => (
  - ${{feature}}
))}}

**{{frontmatter.detailedSpecs}}**

### Pr√™t √† Transformer Votre Exp√©rience ?

Le {product_data.get("brand", "")} {product_data.get("model", "")} est plus qu'un simple produit, c'est une pi√®ce ma√Ætresse pour tout setup s√©rieux. Il allie design, performance et technologies de pointe pour satisfaire les utilisateurs les plus exigeants.

{{/* Le bouton CTA principal - Stylez-le via CSS */}}
<a href={{frontmatter.affiliateLink}} target="_blank" rel="sponsored noopener noreferrer" class="cta-button">
  {{frontmatter.ctaText}}
</a>

*En tant que Partenaire Amazon, je r√©alise un b√©n√©fice sur les achats remplissant les conditions requises.*"""

        return markdown_template

    def process_single_url(self, url):
        """Traite UNE SEULE URL et g√©n√®re UNE fiche produit."""
        print(f"üöÄ Traitement de l'URL : {url}")
        
        # Scraper l'article
        article_data = self.scrape_article(url)
        
        if article_data is None:
            print(f"‚ùå Impossible de r√©cup√©rer l'article de {url}")
            return None
        
        print(f"‚úÖ Article r√©cup√©r√© avec succ√®s : {article_data['title']}")
        
        # G√©n√©rer la fiche produit
        product_sheet = self.generate_product_sheet(article_data)
        
        return product_sheet

    def process_all_urls(self, urls):
        """Traite TOUTES les URLs et g√©n√®re UNE fiche par URL."""
        print(f"üöÄ D√©marrage du traitement de {len(urls)} URL(s)...")
        
        results = []
        
        for i, url in enumerate(urls, 1):
            print(f"\n--- Traitement {i}/{len(urls)} ---")
            
            # Traiter chaque URL individuellement
            product_sheet = self.process_single_url(url)
            
            if product_sheet:
                # Laisser save_to_file g√©n√©rer le nom bas√© sur brand et model
                filepath = self.save_to_file(product_sheet)
                
                if filepath:
                    results.append({
                        'url': url,
                        'filename': filepath,
                        'success': True
                    })
                else:
                    results.append({
                        'url': url,
                        'filename': None,
                        'success': False
                    })
            else:
                print(f"‚ùå Impossible de g√©n√©rer la fiche produit pour {url}")
                results.append({
                    'url': url,
                    'filename': None,
                    'success': False
                })
            
            # Pause entre les URLs pour √©viter de surcharger les serveurs
            if i < len(urls):
                print("‚è≥ Pause de 2 secondes avant l'URL suivante...")
                time.sleep(2)
        
        return results

    def _slugify(self, text):
        """Convertit un texte en slug (caract√®res simples, sans accents, avec tirets)."""
        # Convertir en minuscules
        text = text.lower()
        # Remplacer les caract√®res accentu√©s
        text = re.sub(r'[√†√°√¢√£√§√ß√®√©√™√´√¨√≠√Æ√Ø√±√≤√≥√¥√µ√∂√π√∫√ª√º√Ω√ø]', 
                     lambda m: 'aaaaaceeeeiiiinooooouuuuyy'['√†√°√¢√£√§√ß√®√©√™√´√¨√≠√Æ√Ø√±√≤√≥√¥√µ√∂√π√∫√ª√º√Ω√ø'.index(m.group())], 
                     text)
        # Remplacer tout ce qui n'est pas alphanum√©rique par des tirets
        text = re.sub(r'[^a-z0-9]+', '-', text)
        # Supprimer les tirets en d√©but et fin
        text = text.strip('-')
        # R√©duire les tirets multiples
        text = re.sub(r'-+', '-', text)
        return text

    def save_to_file(self, content, filename=None):
        """Sauvegarde le contenu dans un fichier."""
        # Cr√©er le dossier de sortie s'il n'existe pas
        output_dir = "./fiche"
        os.makedirs(output_dir, exist_ok=True)
        
        if not filename:
            try:
                # Extraire la marque et le mod√®le du contenu markdown
                brand_match = re.search(r"brand: '([^']+)'", content)
                model_match = re.search(r"model: '([^']+)'", content)
                
                if brand_match and model_match:
                    brand = brand_match.group(1)
                    model = model_match.group(1)
                    
                    # Slugifier la marque et le mod√®le
                    brand_slug = self._slugify(brand)
                    model_slug = self._slugify(model)[:40]  # Limiter la longueur du mod√®le si n√©cessaire
                    
                    # Cr√©er le nom de fichier
                    filename = f"fiche-{brand_slug}-{model_slug}.mdx"
                else:
                    # Fallback si on ne trouve pas la marque ou le mod√®le
                    timestamp = datetime.now().strftime("%Y%m%d")
                    filename = f"fiche-{timestamp}.mdx"
            except Exception as e:
                print(f"‚ö†Ô∏è Impossible d'extraire la marque ou le mod√®le: {e}")
                timestamp = datetime.now().strftime("%Y%m%d")
                filename = f"fiche-{timestamp}.mdx"
        
        # Construire le chemin complet
        filepath = os.path.join(output_dir, os.path.basename(filename))
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"‚úÖ Fiche produit sauvegard√©e : {filepath}")
            return filepath
        except Exception as e:
            print(f"‚ùå Erreur lors de la sauvegarde : {e}")
            return None


def main():
    parser = argparse.ArgumentParser(description='G√©n√®re des fiches produits √† partir d\'articles web')
    parser.add_argument('--urls-file', '-f', default='urlfiche.txt', help='Fichier contenant les URLs (d√©faut: urlfiche.txt)')
    parser.add_argument('--single-url', '-u', help='Traiter une seule URL directement')
    
    args = parser.parse_args()
    
    # Initialiser le scraper
    try:
        scraper = ProductScraper()
    except ValueError as e:
        print(e)
        sys.exit(1)
    
    # D√©terminer les URLs √† traiter
    if args.single_url:
        urls = [args.single_url]
        print(f"üéØ Mode URL unique : {args.single_url}")
    else:
        # Charger les URLs depuis le fichier
        urls = scraper.load_urls_from_file(args.urls_file)
        
        if not urls:
            print(f"‚ùå Aucune URL valide trouv√©e dans {args.urls_file}")
            print(f"üí° Cr√©ez le fichier {args.urls_file} avec une URL par ligne.")
            sys.exit(1)
    
    # Traiter les URLs
    try:
        results = scraper.process_all_urls(urls)
        
        # Afficher le r√©sum√©
        successful = [r for r in results if r['success']]
        failed = [r for r in results if not r['success']]
        
        print(f"\nüéâ Traitement termin√© !")
        print(f"‚úÖ Fiches g√©n√©r√©es avec succ√®s : {len(successful)}")
        print(f"‚ùå √âchecs : {len(failed)}")
        
        if successful:
            print(f"\nüìÑ Fichiers g√©n√©r√©s :")
            for result in successful:
                print(f"  - {result['filename']}")
        
        if failed:
            print(f"\n‚ö†Ô∏è  URLs ayant √©chou√© :")
            for result in failed:
                print(f"  - {result['url']}")
            
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Arr√™t demand√© par l'utilisateur.")
        sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå Erreur inattendue : {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()